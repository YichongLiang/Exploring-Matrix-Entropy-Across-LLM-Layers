{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from transformer_lens import HookedTransformer\n",
    "from muutils.dictmagic import condense_tensor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(R):\n",
    "    with torch.no_grad():\n",
    "        mean = R.mean(dim=0)\n",
    "        R = R - mean\n",
    "        norms = torch.norm(R, p=2, dim=1, keepdim=True)\n",
    "        R = R/norms\n",
    "    return R\n",
    "\n",
    "def cal_cov(R):\n",
    "    with torch.no_grad():\n",
    "        Z = torch.nn.functional.normalize(R, dim=1)\n",
    "        print(Z.shape)\n",
    "        A = torch.matmul(Z.T, Z)/Z.shape[0]\n",
    "    return A\n",
    "\n",
    "def cal_entropy(A):\n",
    "    with torch.no_grad():\n",
    "        eig_val = torch.svd(A / torch.trace(A))[1] \n",
    "        entropy = - (eig_val * torch.log(eig_val)).nansum().item()\n",
    "        normalized_entropy = entropy/math.log(A.shape[0])\n",
    "    return normalized_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"good job\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model tiny-stories-3M into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "MODEL: HookedTransformer = HookedTransformer.from_pretrained(\"tiny-stories-3M\")\n",
    "#MODEL: HookedTransformer = HookedTransformer.from_pretrained(\"gpt2\")\n",
    "TOKENIZER = MODEL.tokenizer\n",
    "\n",
    "input_ids = TOKENIZER.encode(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_model=128, n_layers=8, n_heads=16, d_vocab=50257\n"
     ]
    }
   ],
   "source": [
    "d_model: int = MODEL.cfg.d_model\n",
    "n_layers: int = MODEL.cfg.n_layers\n",
    "n_heads: int = MODEL.cfg.n_heads\n",
    "d_vocab: int = MODEL.cfg.d_vocab\n",
    "print(f\"{d_model=}, {n_layers=}, {n_heads=}, {d_vocab=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11274, 1693]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 50257])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL(torch.tensor(input_ids)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 50257])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL(prompt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = MODEL.run_with_cache(\n",
    "    \"test text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blocks:\n",
      "  '[0-7]':\n",
      "    ln1:\n",
      "      hook_scale: (1, 3, 1)\n",
      "      hook_normalized: (1, 3, 128)\n",
      "    attn:\n",
      "      '[hook_q, hook_k, hook_v, hook_z]': (1, 3, 16, 8)\n",
      "      '[hook_attn_scores, hook_pattern]': (1, 16, 3, 3)\n",
      "    ln2:\n",
      "      hook_scale: (1, 3, 1)\n",
      "      hook_normalized: (1, 3, 128)\n",
      "    mlp:\n",
      "      '[hook_pre, hook_post]': (1, 3, 512)\n",
      "    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (1,\n",
      "      3, 128)\n",
      "ln_final:\n",
      "  hook_scale: (1, 3, 1)\n",
      "  hook_normalized: (1, 3, 128)\n",
      "'[hook_embed, hook_pos_embed]': (1, 3, 128)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(condense_tensor_dict(cache, return_format=\"yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/08/h4ttblxd6m5fjbzyhmxs4bk00000gn/T/ipykernel_75155/2033687359.py:18: UserWarning: The operator 'aten::linalg_svd' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  eig_val = torch.svd(A / torch.trace(A))[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13609095266154592\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "   R = cache[\"blocks.0.hook_resid_post\"][0]\n",
    "   R = normalize(R)\n",
    "   A = cal_cov(R)\n",
    "   Entropy = cal_entropy(A) \n",
    "    \n",
    "print(Entropy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
